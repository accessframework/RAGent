{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e737e92-da7a-4136-8aa6-0a9d2bf59c2a",
   "metadata": {},
   "source": [
    "# Demo related to the paper \"RAGent: Retrieval-based Access Control Policy Generation\"\n",
    "\n",
    "\n",
    "> Estimated time: 30 minutes\n",
    "\n",
    "In this demo, we utilize RAGent to identify NLACPs from real-world high-level requirements of HotCRP.com privacy policies and translate them into access control policies.\n",
    "\n",
    "---\n",
    "\n",
    "<details>\n",
    "  <summary>HotCRP Privacy Policy</summary>\n",
    "\n",
    "  This page explains how <u>[HotCRP.com](https://hotcrp.com/)</u> uses the data you provide and how you can control that use. The open-source HotCRP software also runs on sites we don’t manage; this policy applies only to sites with domains ending in “.hotcrp.com”.  \n",
    "(Updated 2020-08-24)  \n",
    "\n",
    "# What we store and why\n",
    "\n",
    "The HotCRP.com service handles the submission and review of *artifacts*, such as scientific and engineering publications. In most cases, HotCRP.com does not own the data you provide. Rather, we broker that data on behalf of scholarly societies, such as ACM and USENIX; on behalf of conference review boards; and on behalf of other site managers.\n",
    "- We store **Submission artifacts**, including submissions (such as PDFs), metadata (such as titles and author lists), and supplementary information (such as topics and other uploaded files).\n",
    "- We store **Review artifacts**, including reviews, discussions, and associated uploaded files.\n",
    "- We store **Configuration settings** used by site managers to configure a site.\n",
    "- We store **Profile data**, such as user names, affiliations, email addresses, topic interests, and review preferences.\n",
    "- We store **Demographic data**, such as user gender identity and approximate age.\n",
    "- We store **Browsing data**, such as logs of which sites a computer has accessed.\n",
    "\n",
    "# Controlling your information\n",
    "\n",
    "**Submission and review artifacts**: When you submit artifacts to a HotCRP.com site, you give that site’s managers (e.g., the program chairs) permission to store and view those artifacts indefinitely. You give the site managers permission to distribute the artifacts at their discretion, for review or other purposes. Finally, you give HotCRP.com permission to process the artifacts and to store them indefinitely. Site managers control who can access artifacts, and HotCRP.com doesn’t share artifacts except as site managers allow. However, we may publicize aggregate information that cannot be traced to any site or user, such as total submission counts across all sites. If an artifact was submitted in error, you can request its permanent deletion. Such requests should be directed to the relevant site managers (e.g., program chairs).\n",
    "\n",
    "**Configuration settings** are stored by HotCRP.com indefinitely. Site managers may request the deletion of a HotCRP.com site, if allowed by their site agreements with HotCRP.com. This will delete all associated site data, including submission and review artifacts.\n",
    "\n",
    "**Profile data** is stored in several ways.\n",
    "- Every HotCRP.com user has a **global profile**, which includes name, email, and affiliation, and other information.\n",
    "- Upon submitting an artifact to a HotCRP.com site, a user gains a **site profile** for that site. This generally contains the same information as the global profile, but it can differ. (For example, changes to a global user affiliation only affect future site profiles.)\n",
    "\n",
    "Users can update their profiles at any time. A site manager can also create a profile for a user, for example by inviting them to join a conference program committee. Contact us if you want your global profile deleted and your site profiles disabled.  \n",
    "\n",
    "**Demographic data** is stored in user global profiles, and can only be modified by users themselves (never by site managers). Users control what demographic data is stored and how demographic data is shared using the Profile \\> Demographics tab on any HotCRP.com site. Information shared only with scholarly societies is provided directly to those societies and cannot be accessed by site managers. Information that is explicitly shared with site managers may also be analyzed by HotCRP.com, but will be published only in aggregate, such as in terms of percentages of active HotCRP.com users.  \n",
    "\n",
    "**Browsing data** is stored for up to a month and per-user browsing data is never shared. We may store and share aggregate information such as total page loads. Misbehavior, such as denial-of-service attacks and attempts to crack a user’s password, may be publicized and may be preserved indefinitely.\n",
    "\n",
    "\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "Finally, before we begin, since LLaMa 3 8B is a gated model, you have visit [huggingface](https://huggingface.co/meta-llama/Meta-Llama-3-8B) and get the access to the base model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58748a36-443b-4423-ad49-14e92284f998",
   "metadata": {},
   "source": [
    "## Install and import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6667e6d-6e26-4f8a-971e-545b7f9ac4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U spacy-experimental\n",
    "!pip install https://github.com/explosion/spacy-experimental/releases/download/v0.6.1/en_coreference_web_trf-3.4.0a2-py3-none-any.whl\n",
    "!pip install transformers==4.30.2\n",
    "!pip install nltk\n",
    "!pip install gdown==5.1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83da017-447d-440a-82be-cc3547eb99f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6574aa9e-cc3e-4aa6-92a2-3f033cc5bc00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"0\"\n",
    "\n",
    "import spacy\n",
    "import json\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# Change the following path appropriately\n",
    "PATH_DEMO_DATA = 'privacy_hotcrp.md'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49311f26-7596-4b0d-b33d-af069e97dd08",
   "metadata": {},
   "source": [
    "## Pre-processing the input privacy policy document\n",
    "\n",
    "This involves seperating the privacy policy document into paragraphs, resolving co-references in each paragraph, and segmenting each paragraph into individual sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657e8921-e3d6-40a3-860f-17704889eb4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_coreference_web_trf')\n",
    "\n",
    "def resolve_references(sent):\n",
    "    \"\"\"Function for resolving references with the coref ouput\n",
    "    doc (Doc): The Doc object processed by the coref pipeline\n",
    "    RETURNS (str): The Doc string with resolved references\n",
    "    \"\"\"\n",
    "\n",
    "    doc = nlp(sent)\n",
    "    token_mention_mapper = {}\n",
    "    output_string = \"\"\n",
    "    clusters = [\n",
    "        val for key, val in doc.spans.items() if key.startswith(\"coref_cluster\")\n",
    "    ]\n",
    "\n",
    "    for cluster in clusters:\n",
    "        first_mention = cluster[0]\n",
    "        for mention_span in list(cluster)[1:]:\n",
    "            token_mention_mapper[mention_span[0].idx] = first_mention.text + mention_span[0].whitespace_\n",
    "\n",
    "            for token in mention_span[1:]:\n",
    "                token_mention_mapper[token.idx] = \"\"\n",
    "\n",
    "    for token in doc:\n",
    "        if token.idx in token_mention_mapper:\n",
    "            output_string += token_mention_mapper[token.idx]\n",
    "        else:\n",
    "            output_string += token.text + token.whitespace_\n",
    "\n",
    "    return output_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4ae1e9-17a2-4cee-8069-e054b8151e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(PATH_DEMO_DATA, 'r+') as f:\n",
    "    content = f.read().replace('*','').replace('#','').replace('-','').replace('\\xa0',' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3c6ded-a56d-4e98-b6f4-73110fdc3567",
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraphs = content.split('\\n\\n')\n",
    "coref_resolved = [resolve_references(k) for k in paragraphs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687a9f35-b309-4784-a9b1-7742d9a64e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_lines = []\n",
    "for p in coref_resolved:\n",
    "    preprocessed_lines.extend(p.split('\\n'))\n",
    "sents = []\n",
    "for p in preprocessed_lines:\n",
    "    sents.extend(sent_tokenize(p))\n",
    "\n",
    "with open('high_level_requirements.json', 'w') as f:\n",
    "  json.dump(sents, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b9864d-5b25-405f-9b9c-27b66839f282",
   "metadata": {},
   "source": [
    "## Running RAGent\n",
    "\n",
    "After identifying individual sentences, then we can run RAGent on these sentences. To this end, we first have to install and import extra libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe370b1f-4f35-4938-9847-6194dacc7b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U transformers\n",
    "!pip install -U peft\n",
    "!pip install bitsandbytes\n",
    "!pip install einops\n",
    "!pip install captum\n",
    "!pip install langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14730a4a-04eb-4df7-b7aa-5b2786b45331",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"3\"\n",
    "\n",
    "import torch\n",
    "from peft import PeftModel\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    BertForSequenceClassification,\n",
    "    BertTokenizerFast\n",
    ")\n",
    "from captum.attr import LayerIntegratedGradients, visualization\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = 'cuda:0'\n",
    "\n",
    "CACHE = '/data/sjay950/huggingface/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4342b76-8ad5-4584-9231-d064ca3fafdd",
   "metadata": {},
   "source": [
    "### Loading the NLACP identification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68a6e73-e9e5-4b74-9c87-c4280e8410ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "ID_MODEL_NAME = '../checkpoints/identification/overall/checkpoint/'\n",
    "NUM_CLASSES = 2\n",
    "\n",
    "id_model = BertForSequenceClassification.from_pretrained(ID_MODEL_NAME, num_labels=NUM_CLASSES).to(device)\n",
    "id_tokenizer = BertTokenizerFast.from_pretrained(ID_MODEL_NAME)\n",
    "\n",
    "id_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf2d2d3-1ea2-4b7d-8585-edfa953c87d6",
   "metadata": {},
   "source": [
    "### Loading the access control policy generation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deeee567-b08d-4a0f-9b7a-6249cc1d8b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "GEN_MODEL_NAME = 'meta-llama/Meta-Llama-3-8B-Instruct'\n",
    "GEN_MODEL_CKPT = '../checkpoints/generation/overall/checkpoint/'\n",
    "\n",
    "model_kwargs = dict(\n",
    "    trust_remote_code=True,\n",
    "    attn_implementation=\"flash_attention_2\",  # loading the model with flash-attenstion support\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    cache_dir = CACHE\n",
    ")\n",
    "base_model = AutoModelForCausalLM.from_pretrained(GEN_MODEL_NAME, **model_kwargs)\n",
    "\n",
    "gen_model = PeftModel.from_pretrained(base_model, GEN_MODEL_CKPT).to('cuda:0')\n",
    "\n",
    "gen_tokenizer = AutoTokenizer.from_pretrained(GEN_MODEL_NAME, use_fast=True, cache_dir=CACHE)\n",
    "gen_tokenizer.pad_token = gen_tokenizer.eos_token\n",
    "gen_tokenizer.padding_side = 'left'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40c7523-7952-48b1-a3db-5fdd904256c0",
   "metadata": {},
   "source": [
    "### Loading the access control policy verification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f5760e-5803-4878-97c9-d73f77c8a01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "VER_MODEL = \"facebook/bart-large\"\n",
    "VER_MODEL_CKPT = \"../checkpoints/verification/checkpoint/\"\n",
    "\n",
    "ver_tokenizer = AutoTokenizer.from_pretrained(VER_MODEL, cache_dir = CACHE)\n",
    "\n",
    "ver_model = AutoModelForSequenceClassification.from_pretrained(VER_MODEL_CKPT).to(\n",
    "    \"cuda:0\"\n",
    ")\n",
    "ver_model = ver_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32217801-e490-4bca-9183-8df36abde518",
   "metadata": {},
   "source": [
    "### Supporting function definition for explaining the verification result through feature attribution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242d6de7-dbb6-428a-887a-00391fc56f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fwd_function(input_ids, attention_mask):\n",
    "\n",
    "    inp = {'input_ids': input_ids.to(device), 'attention_mask': attention_mask.to(device)}\n",
    "\n",
    "    # with torch.no_grad():\n",
    "    pred = ver_model(**inp).logits\n",
    "\n",
    "    return pred\n",
    "\n",
    "\n",
    "def construct_input_ref(s,l, ref_token_id, sep_token_id, bos_token_id):\n",
    "    toks = ver_tokenizer.encode(s)\n",
    "    tokl = ver_tokenizer.encode(l)\n",
    "\n",
    "    input_ids = ver_tokenizer.encode(s,l)\n",
    "\n",
    "    # construct reference token ids\n",
    "    ref_input_ids = [bos_token_id] + [ref_token_id] * (len(toks)-2) + [sep_token_id] + [sep_token_id] + (len(tokl)-2)*[ref_token_id] + [sep_token_id]\n",
    "\n",
    "    return torch.tensor([input_ids], device=device), torch.tensor([ref_input_ids], device=device)\n",
    "\n",
    "def construct_attention_mask(input_ids):\n",
    "    return torch.ones_like(input_ids)\n",
    "\n",
    "\n",
    "def clean(tokens):\n",
    "    l = []\n",
    "    for t in tokens:\n",
    "        if t.startswith('Ġ'):\n",
    "            l.append(t[1:])\n",
    "        else:\n",
    "            l.append(t)\n",
    "    return l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a32c76-5723-4468-9314-6f1678ca29f4",
   "metadata": {},
   "source": [
    "The below cell defines the layer integrated gradients instance using the verifier's forward methos and its embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e432d6-6e9f-49cc-89fd-8670e30c24ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "lig = LayerIntegratedGradients(fwd_function, ver_model.base_model.encoder.embed_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a8bfa5-ab1a-4b4f-8f11-69f8cecdd0a8",
   "metadata": {},
   "source": [
    "The below cell defines functions need to store and visualize feature attributions by the Captum library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5edec40-e58f-4a07-b2c0-47df0962094f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import ID2AUGS\n",
    "\n",
    "vis_data_records_ig = []\n",
    "\n",
    "def add_attributions_to_visualizer(attributions, text, pred, pred_ind, label, delta, vis_data_records):\n",
    "    attributions = attributions.sum(dim=2).squeeze(0)\n",
    "    attributions = attributions / torch.norm(attributions)\n",
    "    attributions = attributions.cpu().detach().numpy()\n",
    "\n",
    "    # storing couple samples in an array for visualization purposes\n",
    "    vis_data_records.append(visualization.VisualizationDataRecord(\n",
    "                            attributions,\n",
    "                            pred,\n",
    "                            ID2AUGS[pred_ind],\n",
    "                            ID2AUGS[label],\n",
    "                            ID2AUGS[pred_ind],\n",
    "                            attributions.sum(),\n",
    "                            text,\n",
    "                            delta))\n",
    "\n",
    "def interpret_sentence(sentence, policy, tokenizer, pred_ind, pred, label):\n",
    "\n",
    "    input_ids, input_ref = construct_input_ref(sentence, policy, tokenizer.pad_token_id, tokenizer.sep_token_id, tokenizer.bos_token_id)\n",
    "\n",
    "    indices = input_ids[0].detach().tolist()\n",
    "    text = clean(tokenizer.convert_ids_to_tokens(indices))\n",
    "\n",
    "    attention_mask = construct_attention_mask(input_ids)\n",
    "\n",
    "    attributions_ig, delta = lig.attribute(inputs=input_ids,\n",
    "                                           baselines=input_ref,\n",
    "                                           additional_forward_args=(attention_mask),\n",
    "                                           n_steps=500,\n",
    "                                           target=pred_ind,\n",
    "                                           internal_batch_size = 4,\n",
    "                                           return_convergence_delta=True)\n",
    "\n",
    "    print('pred: ', ID2AUGS[pred_ind], '(', '%.2f'%pred, ')', ', delta: ', abs(delta))\n",
    "    add_attributions_to_visualizer(attributions_ig, text, pred, pred_ind, label, delta, vis_data_records_ig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7739d99-08c5-49aa-957e-79f4a3269dcc",
   "metadata": {},
   "source": [
    "### Executing RAGentV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9606a2f-d8b2-4a88-adfc-8c89bf0bc597",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('high_level_requirements.json', 'r') as f:\n",
    "    sents = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4528024d-a7be-4e87-9eeb-561ff5ec2ca2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!mkdir -p generations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e729e0-30b8-4f8c-81bc-966954cefd5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import create_vectorstores\n",
    "\n",
    "stores = create_vectorstores('entities.json', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ce1ef2-1d9f-44e2-8672-033218e1c890",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import generate_policy, is_nlacp, convert_to_sent\n",
    "\n",
    "inputs, outputs = [], []\n",
    "\n",
    "id = 0\n",
    "for s in tqdm(sents):\n",
    "    inputs.append(s)\n",
    "    nlacp = is_nlacp(s, id_model, id_tokenizer)\n",
    "    if nlacp:\n",
    "        output, pred_ind, mprob = generate_policy(id, s, gen_model, gen_tokenizer, ver_model, ver_tokenizer, with_ents=True, store=stores)\n",
    "        if pred_ind != 11:\n",
    "            pp, _ = convert_to_sent(str(output))\n",
    "            interpret_sentence(s, pp, ver_tokenizer, pred_ind, mprob, pred_ind)\n",
    "\n",
    "    else:\n",
    "        output = 'Not an ACP'\n",
    "    \n",
    "    outputs.append(output)\n",
    "    id+=1\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'inputs': inputs,\n",
    "    'outputs': outputs\n",
    "})\n",
    "\n",
    "df.to_csv('results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553daad9-69ab-490f-ad09-03f55d32c570",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = visualization.visualize_text(vis_data_records_ig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769d8198-51d3-454a-a6ae-5d2d8f924d9e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
