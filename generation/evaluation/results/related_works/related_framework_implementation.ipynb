{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea58b75a-7149-4a6b-a8b6-f00df6d324aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "\n",
    "# from practnlptools.tools import Annotator\n",
    "# annotator = Annotator()\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70268eef-b6b3-4327-b9db-099c439aa2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "collected = pd.read_csv('../../../../data/document_folds/collected_acp.csv')\n",
    "cyber = pd.read_csv('../../../../data/document_folds/cyber_acp.csv')\n",
    "ibm = pd.read_csv('../../../../data/document_folds/ibm_acp.csv')\n",
    "t2p = pd.read_csv('../../../../data/document_folds/t2p_acp.csv')\n",
    "acre = pd.read_csv('../../../../data/document_folds/acre_acp.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdaddf44-986d-407e-adab-cb0c06cc265a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Narouei et al."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "32c3418e-9f61-416d-8fe5-90877852e62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess(key, val):\n",
    "    \n",
    "    l = ['any', 'all', 'every','the']\n",
    "    \n",
    "    val = val.replace('_',',').replace('-', '').replace(\"'\", \"\").replace('’', '').replace(\"”\", \"\").replace(\"“\",\"\")\n",
    "    \n",
    "    if key == 'subject' or key == 'resource':\n",
    "        for k in l:\n",
    "            if val.split(' ')[0] == k:\n",
    "                val = ' '.join(val.split(' ')[1:])\n",
    "    \n",
    "    if len(val)>=2 and val[-1] == 's' and val[-2] != 's':\n",
    "        val = val[:-1]\n",
    "        \n",
    "    return ' '.join(val.split())\n",
    "\n",
    "def get_senna_policies(sent):\n",
    "    senna_srl = annotator.getAnnotations(sent)['srl']\n",
    "    policies = []\n",
    "    for srl in senna_srl:\n",
    "        \n",
    "        if 'V' not in srl:\n",
    "            continue\n",
    "        else:\n",
    "            subjects, resources = [],[]\n",
    "            action = srl['V']\n",
    "            if 'A0' in srl:\n",
    "                ners = nlp(srl['A0']).ents\n",
    "                subjects = [srl['A0']] if len(ners)==0 else list(ners)\n",
    "            if 'A1' in srl:\n",
    "                nerr = nlp(srl['A1']).ents\n",
    "                resources = [srl['A1']] if len(nerr)==0 else list(nerr) \n",
    "    \n",
    "            for s in subjects:\n",
    "                for r in resources:\n",
    "                    p = {'subject': postprocess('subject', str(s).lower()), 'action': action, 'resource': postprocess('resource', str(r).lower())}\n",
    "                    policies.append(p)\n",
    "\n",
    "    return policies\n",
    "\n",
    "\n",
    "def create_srls(sent):\n",
    "\n",
    "    p = get_senna_policies(sent)\n",
    "    srls = {}\n",
    "    for pp in p:\n",
    "        action = pp['action']\n",
    "        if action not in srls:\n",
    "            srls[action] = {'subject': [], 'resource': []}\n",
    "            if str(pp['subject'])!='none':\n",
    "                srls[action]['subject'].append(pp['subject'])\n",
    "            if str(pp['resource'])!='none':\n",
    "                srls[action]['resource'].append(pp['resource'])\n",
    "        else:\n",
    "            if str(pp['subject'])!='none':\n",
    "                srls[action]['subject'].append(pp['subject'])\n",
    "            if str(pp['resource'])!='none':\n",
    "                srls[action]['resource'].append(pp['resource'])\n",
    "\n",
    "    return srls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050f870a-3826-4908-85bf-dc65047776e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "pred_pols = []\n",
    "for sent in collected['input'].to_list():\n",
    "    pred_pols.append(create_srls(sent))\n",
    "with open('senna/collected_senna_pred_srl.json', 'w') as f:\n",
    "    json.dump(pred_pols, f)\n",
    "\n",
    "pred_pols = []\n",
    "for sent in ibm['input'].to_list():\n",
    "    pred_pols.append(create_srls(sent))\n",
    "with open('senna/ibm_senna_pred_srl.json', 'w') as f:\n",
    "    json.dump(pred_pols, f)\n",
    "\n",
    "pred_pols = []\n",
    "for sent in cyber['input'].to_list():\n",
    "    pred_pols.append(create_srls(sent))\n",
    "with open('senna/cyber_senna_pred_srl.json', 'w') as f:\n",
    "    json.dump(pred_pols, f)\n",
    "\n",
    "pred_pols = []\n",
    "for sent in t2p['input'].to_list():\n",
    "    pred_pols.append(create_srls(sent))\n",
    "with open('senna/t2p_senna_pred_srl.json', 'w') as f:\n",
    "    json.dump(pred_pols, f)\n",
    "\n",
    "pred_pols = []\n",
    "for sent in acre['input'].to_list():\n",
    "    pred_pols.append(create_srls(sent))\n",
    "with open('senna/acre_senna_pred_srl.json', 'w') as f:\n",
    "    json.dump(pred_pols, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49ef751-96f2-47b1-973c-dfe8edb85559",
   "metadata": {},
   "source": [
    "## Xia et al."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b52f341-5b5a-4ed2-9baa-7173da27623d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from allennlp.predictors.predictor import Predictor\n",
    "predictor_srl = Predictor.from_path(\"https://storage.googleapis.com/allennlp-public-models/structured-prediction-srl-bert.2020.12.15.tar.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ab5b82c-9aa0-4e7b-8b9d-0211aa80753e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "    \n",
    "def DFSUtil(s, visited, l):\n",
    "\n",
    "    visited.add(s)\n",
    "\n",
    "    if (str(s.dep_) == 'ROOT' or str(s.dep_) == 'conj' or str(s.dep_) == 'appos'):\n",
    "        # print(f'1: {s.dep_}: {s.text}')\n",
    "        l.append([s])\n",
    "            \n",
    "    elif (str(s.dep_) == 'amod' or str(s.dep_) == 'compound' or str(s.dep_) == 'nsubj' or str(s.dep_) == 'poss' or str(s.dep_) == 'npadvmod'):\n",
    "        # print(f'2: {s.dep_}: {s.text} - {[w.text for w in s.lefts]} - {[w.text for w in s.rights]}')\n",
    "        if str(s.dep_) == 'compound' or str(s.dep_) == 'npadvmod' or str(s.dep_) == 'amod':\n",
    "            # print(len(l[-1]))\n",
    "            if len(l[-1])>=2:\n",
    "                nexttolast = l[-1][0]\n",
    "                if s.text in [w.text for w in nexttolast.lefts]:\n",
    "                    l[-1].insert(0, s)\n",
    "                else:\n",
    "                    l[-1].insert(-1, s)\n",
    "            else:\n",
    "                l[-1].insert(-1, s)\n",
    "        else:\n",
    "            l[-1].insert(-1, s)\n",
    "                \n",
    "    elif (str(s.dep_) == 'prep' or str(s.dep_) == 'pobj' or str(s.dep_) == 'dobj'):\n",
    "        # print(f'3: {s.dep_}: {s.text}')      \n",
    "        l[-1].append(s)\n",
    " \n",
    "    for neighbour in s.children:\n",
    "        if neighbour not in visited and neighbour.text not in string.punctuation:\n",
    "            DFSUtil(neighbour, visited, l)\n",
    " \n",
    "def DFS(v):\n",
    " \n",
    "    visited = set()\n",
    "    l = []\n",
    "\n",
    "    DFSUtil(v, visited, l)\n",
    "    interans = []\n",
    "    for noun in l:\n",
    "        nl = []\n",
    "        for word in noun:\n",
    "            nl.append(word.text)\n",
    "        interans.append(nl)\n",
    "    ans = [' '.join(k) for k in interans] \n",
    "    return (ans)\n",
    "    \n",
    "    \n",
    "def get_root(doc):\n",
    "    for token in doc:\n",
    "        if (token.dep_ == 'ROOT'):\n",
    "            return token\n",
    "\n",
    "def postprocess(key, val):\n",
    "    \n",
    "    l = ['any', 'all', 'every','the']\n",
    "    \n",
    "    val = val.replace('_',',').replace('-', '').replace(\"'\", \"\").replace('’', '').replace(\"”\", \"\").replace(\"“\",\"\")\n",
    "    \n",
    "    if key == 'subject' or key == 'resource':\n",
    "        for k in l:\n",
    "            if val.split(' ')[0] == k:\n",
    "                val = ' '.join(val.split(' ')[1:])\n",
    "    \n",
    "    if len(val)>=2 and val[-1] == 's' and val[-2] != 's':\n",
    "        val = val[:-1]\n",
    "        \n",
    "    return ' '.join(val.split())\n",
    "\n",
    "# DFS(get_root(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "581cd08c-7237-4de3-9421-b0ccb766aaa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_acp(tokens, srl, use_dfs = True):\n",
    "    prev = \"\"\n",
    "    d = {}\n",
    "    word = []\n",
    "    for t,l in zip(tokens, srl):\n",
    "        \n",
    "        ind = l.split('-')[0]\n",
    "        k = '-'.join(l.split('-')[1:])\n",
    "        if k != prev:\n",
    "            d[prev] = ' '.join(word)\n",
    "            prev = k\n",
    "            word = [t]\n",
    "        if ind == 'I':\n",
    "            word.append(t)\n",
    "            \n",
    "    d[prev] = ' '.join(word)\n",
    "    p = []\n",
    "    sub_doc = nlp(d['ARG0']) if 'ARG0' in d else 'none'\n",
    "    # print(d['ARG0'])\n",
    "    res_doc = nlp(d['ARG1']) if 'ARG1' in d else 'none'\n",
    "    \n",
    "    if use_dfs:\n",
    "        subs = DFS(get_root(sub_doc)) if 'ARG0' in d else ['none']\n",
    "        # print(subs)\n",
    "        ress = DFS(get_root(res_doc)) if 'ARG1' in d else ['none']\n",
    "        \n",
    "    return subs, ress\n",
    "\n",
    "def generate_acp(p, dopostprocess = True):\n",
    "    \n",
    "    tokens = p['words']\n",
    "    acps = []\n",
    "    l = ['feel','look','sound','taste','smell','seem','appear','become','grow','get','turn','fall','go','come','continue','remain','stay',\n",
    "         'aware','can','could','be able to','may','might','must','dare','need','shall','should','be supposed to','ought to','will','would',\n",
    "         'be going to','used to', 'is', 'am', 'are','was','were','have','has','had','do','does','did', 'permit','allow','enable','contain','include','consist'\n",
    "        ]\n",
    "    for v in p['verbs']:\n",
    "        if v['verb'] not in l:\n",
    "            docv = nlp(v['verb'])\n",
    "            act = []\n",
    "            for t in docv:\n",
    "                act.append(t.lemma_)\n",
    "            action = ' '.join(act)\n",
    "            \n",
    "            # print(v['verb'], v['tags'])\n",
    "            sub,res = collect_acp(tokens, v['tags']) # Reurns subjects and resources for a given action\n",
    "            # print(action, sub, res)\n",
    "            for s in sub:\n",
    "                for r in res:\n",
    "                    if dopostprocess:\n",
    "                        acps.append({'subject': postprocess('subject', s.lower()), 'action': action, 'resource': postprocess('resource', r.lower())})\n",
    "                    else:\n",
    "                        docs = nlp(s.lower())\n",
    "                        sub = []\n",
    "                        for t in docs:\n",
    "                            sub.append(t.lemma_)\n",
    "                        subject = ' '.join(sub)\n",
    "\n",
    "                        docr = nlp(r.lower())\n",
    "                        res = []\n",
    "                        for t in docr:\n",
    "                            res.append(t.lemma_)\n",
    "                        resource = ' '.join(res)\n",
    "\n",
    "                        acps.append({'subject': subject, 'action': action, 'resource': resource})\n",
    "                        \n",
    "                   \n",
    "    return acps \n",
    "\n",
    "\n",
    "def create_srls_xia(sent, dopostprocess = True):\n",
    "\n",
    "    p = generate_acp(predictor_srl.predict(sentence = sent), dopostprocess)\n",
    "    srls = {}\n",
    "    for pp in p:\n",
    "        action = pp['action']\n",
    "        if action not in srls:\n",
    "            srls[action] = {'subject': [], 'resource': []}\n",
    "            if str(pp['subject'])!='none':\n",
    "                srls[action]['subject'].append(pp['subject'])\n",
    "            if str(pp['resource'])!='none':\n",
    "                srls[action]['resource'].append(pp['resource'])\n",
    "        else:\n",
    "            if str(pp['subject'])!='none':\n",
    "                srls[action]['subject'].append(pp['subject'])\n",
    "            if str(pp['resource'])!='none':\n",
    "                srls[action]['resource'].append(pp['resource'])\n",
    "\n",
    "    return srls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "17f2389c-dbfb-4ca8-bb47-ec0bbb15e1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "pred_pols = []\n",
    "for sent in collected['input'].to_list():\n",
    "    try:\n",
    "        pred_pols.append(create_srls_xia(sent))\n",
    "    except:\n",
    "        print(sent)\n",
    "        print(generate_acp(predictor_srl.predict(sentence = sent)))\n",
    "        break\n",
    "\n",
    "with open('xia/collected_xia_pred_srl.json', 'w') as f:\n",
    "    json.dump(pred_pols, f)\n",
    "\n",
    "pred_pols = []\n",
    "for sent in cyber['input'].to_list():\n",
    "    try:\n",
    "        pred_pols.append(create_srls_xia(sent))\n",
    "    except:\n",
    "        print(sent)\n",
    "        print(generate_acp(predictor_srl.predict(sentence = sent)))\n",
    "        break\n",
    "\n",
    "with open('xia/cyber_xia_pred_srl.json', 'w') as f:\n",
    "    json.dump(pred_pols, f)\n",
    "\n",
    "pred_pols = []\n",
    "for sent in ibm['input'].to_list():\n",
    "    try:\n",
    "        pred_pols.append(create_srls_xia(sent))\n",
    "    except:\n",
    "        print(sent)\n",
    "        print(generate_acp(predictor_srl.predict(sentence = sent)))\n",
    "        break\n",
    "\n",
    "with open('xia/ibm_xia_pred_srl.json', 'w') as f:\n",
    "    json.dump(pred_pols, f)\n",
    "\n",
    "pred_pols = []\n",
    "for sent in acre['input'].to_list():\n",
    "    try:\n",
    "        pred_pols.append(create_srls_xia(sent))\n",
    "    except:\n",
    "        print(sent)\n",
    "        print(generate_acp(predictor_srl.predict(sentence = sent)))\n",
    "        break\n",
    "\n",
    "with open('xia/acre_xia_pred_srl.json', 'w') as f:\n",
    "    json.dump(pred_pols, f)\n",
    "\n",
    "pred_pols = []\n",
    "for sent in t2p['input'].to_list():\n",
    "    try:\n",
    "        pred_pols.append(create_srls_xia(sent))\n",
    "    except:\n",
    "        print(sent)\n",
    "        print(generate_acp(predictor_srl.predict(sentence = sent)))\n",
    "        break\n",
    "\n",
    "with open('xia/t2p_xia_pred_srl.json', 'w') as f:\n",
    "    json.dump(pred_pols, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153d5bf0-7f41-4b69-99e9-94ad0b2f1c8e",
   "metadata": {},
   "source": [
    "### Evaluating the demo results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24beaaa9-b0df-4da1-bbe0-f566a04679ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('../../../../demo/high_level_requirements.json','r') as f:\n",
    "    sents = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "646c34a2-3afe-4dc6-be50-f54ac7f285aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_pols = []\n",
    "for sent in sents:\n",
    "    try:\n",
    "        pred_pols.append(create_srls_xia(sent))\n",
    "    except:\n",
    "        print(sent)\n",
    "        print(generate_acp(predictor_srl.predict(sentence = sent)))\n",
    "        break\n",
    "\n",
    "with open('../../../../demo/results/demo_pred_srl_xia.json', 'w') as f:\n",
    "    json.dump(pred_pols, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043e72f0-e4ca-4c8b-abdc-37a03867832f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
